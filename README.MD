# Reinforced Outrun

## Personnal Motivation

Sure, that are pretty good library to practice your reinforment learning skills in the market, just like OpenAIGym. The problem was that, I personally have never played Atari in my life, but instead, have played a lot of OutRun. These factor made me try to create a whole new reinforcement learning engine, where I could load any Genesis game or any emulated capable game and train an AI over it. Of course, you may think that this project could be emulated into a GCP or an AWS and all episodes could trained faster, in a blink of an eye, but that`s not the point of this engine. This engine is created for you to observe each episode with your own eyes, check if your reward function work on the go, analyse and learn. That`s the point of this engine.
In this particular project everuthing is tweaked to work for OutRun, but if you change the reward, game over and controls functions, it should work for every Genesis game you like.

## Preparing your Engine

This project is intended to run o Linux machines. And it is divided in two parts:

- VM/Dedicated Computer
- Trainer API

### VM/Dedicated Computer:

In order to make your own RL VM or transform your old computer into a dedicated RL machine, you have to install in your machine (recommend) the following python Libraries:
 
```
tensorflow
pyautogui
mss
cv2
asyncio
evdev
zlib
numpy
multiprocessing
```

And, of course, have microsoft Xbox 360 controller (or any other controller that emulates XBOX 360 Buttons). This is because the program always hooks the controller in order to output the desired buttons generated by the neural network.
The controller is also important if you want to play (very important) and pre-train your model (also very important, without pre train your machine may take weeks to be a resonable player).

All the code uploaded was tested on LinuxMint and may work on any other Debian based OS.

VM/Dedicated Computer Codes:

```
Reinforced_Outrun_VMCODES
```

### Trainer API

The trainer API is just a flask server waiting for the training data to be sent by the VM/Dedicated Computer. 
After the data is sent, it will train and return a new trained model do the VM/Dedicated Computer.

```
tensorflow
flask
cv2
zlib
numpy
multiprocessing
```

Trainer API Codes:

```
TensorflowAPI_RLOutRun
```

## How to play?

On VM/Dedicated Computer:

Example
```
#chmod +0777 /dev/uinput
$python NetworkTrain_Outrun.py -i 192.168.132.1:5000 -s false -r false
```

Explanation:

```
#chmod +0777 /dev/uinput
```

This command is to allow your non root user to access and hook your Xbox360 controller. I personnaly left this command on my root crontab.

```
#python NetworkTrain_Outrun.py -i 192.168.132.1:5000 -s false -r false
```

Arg -i: IP and port of your Trainer API server.

Arg -s: 'true' if you want to save all of your data after the episode, 'false' if not. 

Arg -r: 'true' if you want random play at random times (to generalize your model), 'false' if not.

On Trainer API:

```
$python all_on_server_side.py
```


## How does it work?

Actually, very simple:

- Starting the script the engine initilizes Gens emulator and press 'F8' to load state (load state was saved by me, so you problaby will have to load state)

- The engine analyses the past second to make a prediction for the next 0.2 seconds. So, in theory, actions are changed every 0.2 seconds.

- Engine will analyse if the game is over or not. If it is over, the engine will send all the data to the API and wait.

- Trainer API will caculate rewards and retrain the model.

- The new model will be sent to the RL engine to be ran again.

- RL Engine will load state and start playing with the new model.

Here you can see some schematics of how the it is working:

![Alt text](./Images/Diagram.png?raw=true "Diagram")

Check it out:

<iframe width="560" height="315" src="https://www.youtube.com/embed/EA_NDtStrGc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### Concepts

## Q-Learning

To update the RL Model, Q-Learning algorithm is implemented in the Trainer API. It is needed to build the Q-function (one of the forms of the Bell-Equation) by using the Q-value iteration update:

![Alt text](./Images/q_value_iteration_update.svg?raw=true "QFunction")

# Reward Function

The reward function’s definition is crucial for good learning performance and determines the goal in a reinforcement learning problem. Based on experiments, I’ve built a function which rewards logarithmically based on speed, position and delta points and penalizes when the car is off-road, crashed, etc.

![Alt text](./Images/reward.png?raw=true "Reward")

All the within models to make the engine work area represented in the following image:

![Alt text](./Images/RewardExplained.png?raw=true "ExReward")

### Results

After a few episodes of training we can analyse all the data and it's results:

![Alt text](./Images/MeanScore.png?raw=true "MeanScore")

![Alt text](./Images/RLScore.png?raw=true "RLScore")

![Alt text](./Images/speed.png?raw=true "speed")

![Alt text](./Images/totalScore.png?raw=true "totalScore")

# After a few episodes

This is a result of 2 days training. I have 1050TI and each episode takes more or less 80 seconds, trainig takes about 30 seconds:

<iframe width="560" height="315" src="https://www.youtube.com/embed/RX7NwMnc7As" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
