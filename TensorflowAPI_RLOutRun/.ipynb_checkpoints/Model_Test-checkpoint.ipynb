{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "def save_to_file(objeto, nome_arquivo):\n",
    "    with open(nome_arquivo, 'wb') as output:\n",
    "        pickle.dump(objeto, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_file(nome_arquivo):\n",
    "    with open(nome_arquivo, 'rb') as input:\n",
    "        objeto = pickle.load(input)\n",
    "    return objeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_return_decision(model,images):\n",
    "    \n",
    "    # predict command\n",
    "    imgs = np.array(images).astype(np.uint8)\n",
    "    imgs = np.expand_dims(imgs, axis=0)\n",
    "    imgs = np.swapaxes(imgs,1,2)\n",
    "    imgs = np.swapaxes(imgs,2,3)\n",
    "    print(imgs.shape)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(hashlib.md5(imgs.tobytes()).hexdigest())\n",
    "    except Exception as inst:\n",
    "        print('erro hash')\n",
    "        print(inst)\n",
    "    \"\"\"\n",
    "    \n",
    "    return model.predict(imgs.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = load_file('./Backup/fila_treino_104.0_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON LOADING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open(\"./Backup/modelo_outrun.json\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "model.load_weights(\"./Backup/modelo_outrun.hdf5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 210, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.02521727, -0.02561437,  0.06758901,  0.02815963, -0.01365319,\n",
       "         0.02351109,  0.14803128,  0.11499287,  0.08947708, -0.01735645,\n",
       "         0.04756565,  0.03563783]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capture_return_decision( model, [ process_img(j[0][120:,60:270]) for j in queue[50-5:50]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.02521727, -0.02561437,  0.06758901,  0.02815963, -0.01365319,\n",
       "          0.02351109,  0.14803128,  0.11499287,  0.08947708, -0.01735645,\n",
       "          0.04756565,  0.03563783]], dtype=float32), 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict( np.expand_dims( model_capture_return_images( [ process_img(j[0][120:,60:270]) for j in queue[50-5:50]]) , axis=0 ) ), \\\n",
    "model.predict( np.expand_dims( model_capture_return_images( [ process_img(j[0][120:,60:270]) for j in queue[50-5:50]]) , axis=0 ) ).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02 -0.02  0.06  0.04 -0.01  0.03  0.13  0.11  0.08 -0.02  0.04  0.04]] 6\n",
      "[[ 0.02 -0.02  0.06  0.04 -0.01  0.03  0.13  0.11  0.08 -0.02  0.04  0.04]] 6\n",
      "[[ 0.02 -0.02  0.06  0.04 -0.01  0.03  0.13  0.11  0.08 -0.02  0.04  0.04]] 6\n",
      "[[ 0.02 -0.01  0.06  0.04 -0.01  0.03  0.14  0.11  0.08 -0.02  0.04  0.03]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.05  0.04]] 6\n",
      "[[ 0.04 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.05  0.05]] 6\n",
      "[[ 0.03 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.04  0.04]] 6\n",
      "[[ 0.03 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.01  0.03  0.14  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.01  0.03  0.13  0.11  0.09 -0.    0.03  0.03]] 6\n",
      "[[ 0.03 -0.02  0.06  0.05 -0.02  0.03  0.13  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.01  0.03  0.14  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.14  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.03 -0.02  0.06  0.05 -0.02  0.04  0.13  0.11  0.09 -0.    0.03  0.03]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.01  0.03  0.13  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.01  0.03  0.13  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.01  0.03  0.13  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.02 -0.02  0.06  0.04 -0.01  0.03  0.13  0.11  0.09 -0.    0.04  0.03]] 6\n",
      "[[ 0.03 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.02 -0.02  0.06  0.03 -0.01  0.03  0.14  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.03 -0.02  0.07  0.04 -0.01  0.03  0.14  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.02 -0.02  0.06  0.04 -0.01  0.03  0.14  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.01  0.03  0.13  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.03 -0.03  0.06  0.04 -0.02  0.03  0.14  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.04 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.04  0.05]] 6\n",
      "[[ 0.03 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.04  0.04]] 6\n",
      "[[ 0.04 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.05  0.05]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.04  0.04]] 6\n",
      "[[ 0.04 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.05  0.05]] 6\n",
      "[[ 0.04 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.05  0.05]] 6\n",
      "[[ 0.04 -0.03  0.05  0.04 -0.02  0.03  0.13  0.11  0.1  -0.02  0.05  0.05]] 6\n",
      "[[ 0.03 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.07  0.04 -0.01  0.03  0.14  0.1   0.09 -0.03  0.05  0.05]] 6\n",
      "[[ 0.03 -0.02  0.07  0.04 -0.01  0.03  0.14  0.11  0.09 -0.02  0.05  0.05]] 6\n",
      "[[ 0.02 -0.02  0.07  0.03 -0.01  0.02  0.15  0.12  0.09 -0.02  0.05  0.03]] 6\n",
      "[[ 0.02 -0.02  0.06  0.03 -0.01  0.03  0.15  0.12  0.09 -0.02  0.05  0.03]] 6\n",
      "[[ 0.02 -0.02  0.07  0.02 -0.01  0.02  0.15  0.12  0.09 -0.01  0.05  0.03]] 6\n",
      "[[ 0.03 -0.03  0.06  0.03 -0.02  0.03  0.14  0.11  0.09 -0.02  0.05  0.04]] 6\n"
     ]
    }
   ],
   "source": [
    "#for i in np.arange(5,len(queue)-5,1):\n",
    "for i in np.arange(5,50,1):\n",
    "    #print('Starting to organize data before training... NÂº', i, 'of', len(queue),'samples.', end='\\r')\n",
    "    print(np.round( model.predict( np.expand_dims( model_capture_return_images( [ process_img(j[0][120:,60:270]) for j in queue[i-5:i]]) , axis=0 ) ), 2 ) , queue[i][1].argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Compiled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('./Backup/modelo_outrun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "              optimizer=tf.keras.optimizers.SGD(),\n",
    "              metrics=['mse','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_5 (Lambda)            (None, 100, 210, 5)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 81, 64, 64)        128064    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 81, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 81, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 81, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 40, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 16, 12, 32)        204832    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 8, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 1, 32)          25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2, 1, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 12)                780       \n",
      "=================================================================\n",
      "Total params: 376,396\n",
      "Trainable params: 376,140\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 210, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.02521727, -0.02561437,  0.06758901,  0.02815963, -0.01365319,\n",
       "         0.02351109,  0.14803128,  0.11499287,  0.08947708, -0.01735645,\n",
       "         0.04756565,  0.03563783]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capture_return_decision( model, [ process_img(j[0][120:,60:270]) for j in queue[50-5:50]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.02521727, -0.02561437,  0.06758901,  0.02815963, -0.01365319,\n",
       "          0.02351109,  0.14803128,  0.11499287,  0.08947708, -0.01735645,\n",
       "          0.04756565,  0.03563783]], dtype=float32), 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict( np.expand_dims( model_capture_return_images( [ process_img(j[0][120:,60:270]) for j in queue[50-5:50]]) , axis=0 ).astype(np.float32) ), \\\n",
    "model.predict( np.expand_dims( model_capture_return_images( [ process_img(j[0][120:,60:270]) for j in queue[50-5:50]]) , axis=0 ).astype(np.float32) ).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02 -0.02  0.06  0.04 -0.01  0.03  0.13  0.11  0.08 -0.02  0.04  0.04]] 6\n",
      "[[ 0.02 -0.02  0.06  0.04 -0.01  0.03  0.13  0.11  0.08 -0.02  0.04  0.04]] 6\n",
      "[[ 0.02 -0.02  0.06  0.04 -0.01  0.03  0.13  0.11  0.08 -0.02  0.04  0.04]] 6\n",
      "[[ 0.02 -0.01  0.06  0.04 -0.01  0.03  0.14  0.11  0.08 -0.02  0.04  0.03]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.05  0.04]] 6\n",
      "[[ 0.04 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.05  0.05]] 6\n",
      "[[ 0.03 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.04  0.04]] 6\n",
      "[[ 0.03 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.01  0.03  0.14  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.01  0.03  0.13  0.11  0.09 -0.    0.03  0.03]] 6\n",
      "[[ 0.03 -0.02  0.06  0.05 -0.02  0.03  0.13  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.01  0.03  0.14  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.14  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.03 -0.02  0.06  0.05 -0.02  0.04  0.13  0.11  0.09 -0.    0.03  0.03]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.01  0.03  0.13  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.01  0.03  0.13  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.01  0.03  0.13  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.02 -0.02  0.06  0.04 -0.01  0.03  0.13  0.11  0.09 -0.    0.04  0.03]] 6\n",
      "[[ 0.03 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.02 -0.02  0.06  0.03 -0.01  0.03  0.14  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.03 -0.02  0.07  0.04 -0.01  0.03  0.14  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.02 -0.02  0.06  0.04 -0.01  0.03  0.14  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.01  0.03  0.13  0.11  0.09 -0.01  0.04  0.03]] 6\n",
      "[[ 0.03 -0.03  0.06  0.04 -0.02  0.03  0.14  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.04 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.04  0.05]] 6\n",
      "[[ 0.03 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.04  0.04]] 6\n",
      "[[ 0.04 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.05  0.05]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.04  0.04]] 6\n",
      "[[ 0.04 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.05  0.05]] 6\n",
      "[[ 0.04 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.05  0.05]] 6\n",
      "[[ 0.04 -0.03  0.05  0.04 -0.02  0.03  0.13  0.11  0.1  -0.02  0.05  0.05]] 6\n",
      "[[ 0.03 -0.03  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.02  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.06  0.04 -0.02  0.03  0.13  0.11  0.09 -0.01  0.04  0.04]] 6\n",
      "[[ 0.03 -0.02  0.07  0.04 -0.01  0.03  0.14  0.1   0.09 -0.03  0.05  0.05]] 6\n",
      "[[ 0.03 -0.02  0.07  0.04 -0.01  0.03  0.14  0.11  0.09 -0.02  0.05  0.05]] 6\n",
      "[[ 0.02 -0.02  0.07  0.03 -0.01  0.02  0.15  0.12  0.09 -0.02  0.05  0.03]] 6\n",
      "[[ 0.02 -0.02  0.06  0.03 -0.01  0.03  0.15  0.12  0.09 -0.02  0.05  0.03]] 6\n",
      "[[ 0.02 -0.02  0.07  0.02 -0.01  0.02  0.15  0.12  0.09 -0.01  0.05  0.03]] 6\n",
      "[[ 0.03 -0.03  0.06  0.03 -0.02  0.03  0.14  0.11  0.09 -0.02  0.05  0.04]] 6\n"
     ]
    }
   ],
   "source": [
    "#for i in np.arange(5,len(queue)-5,1):\n",
    "for i in np.arange(5,50,1):\n",
    "    #print('Starting to organize data before training... NÂº', i, 'of', len(queue),'samples.', end='\\r')\n",
    "    print(np.round( model.predict( np.expand_dims( model_capture_return_images( [ process_img(j[0][120:,60:270]) for j in queue[i-5:i]]) , axis=0 ).astype(np.float32) ), 2 ) , queue[i][1].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
